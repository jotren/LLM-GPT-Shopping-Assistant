{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17671b93-415d-42e9-b489-b031a7959ccf",
   "metadata": {},
   "source": [
    "# Creating Image Database\n",
    "\n",
    "Have downloaded a file with training data from [here](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small?select=images). This dataset has a number of images, with some additional tags for each image id. I think in reality, if deployed, this system would need to be a machine learning algorithmn, that scrapes and manages clothes. I am envisage a GPT that is tasked with seperating out clothes into different categories. Here is how the dataset is structured:\n",
    "\n",
    "- id\n",
    "- gender\n",
    "- masterCategory\n",
    "- subCategory\n",
    "- articleType\n",
    "- baseColour\n",
    "- season\n",
    "- usage\n",
    "\n",
    "As well as having this metadata the database will contain embedded vectors relating to the image. My concern is that the data might be too low res, but we can see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260a723a-9173-4bcc-879c-ef671a5bf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the src directory to the Python path\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Now you can import your models\n",
    "from models import Image as ImageModel, Attribute, ItemAttribute, init_db\n",
    "\n",
    "# Required Libraries\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy.orm import sessionmaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17862bd-9417-4f30-a465-3c9e08256a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Database\n",
    "engine = init_db()\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Load CLIP Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Paths\n",
    "images_folder = os.path.join(\"../\", \"data\", \"raw\", \"images\")\n",
    "metadata_file = os.path.join(\"../\", \"data\", \"raw\", \"styles.csv\")\n",
    "embeddings_file = os.path.join(\"..\", \"data\", \"processed\", \"embeddings.npy\")\n",
    "image_ids_file = os.path.join(\"..\", \"data\", \"processed\", \"image_ids.npy\")\n",
    "\n",
    "# Load Metadata\n",
    "metadata = pd.read_csv(metadata_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacda60e-7b08-48cf-8ef1-5128b7435aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Missing Columns\n",
    "required_columns = ['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'use']\n",
    "missing_columns = [col for col in required_columns if col not in metadata.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns in metadata: {missing_columns}\")\n",
    "    exit(1)\n",
    "\n",
    "# Function to Generate Embedding for an Image\n",
    "def generate_embedding(image_path):\n",
    "    try:\n",
    "        image = PILImage.open(image_path).convert(\"RGB\")\n",
    "        image = preprocess(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model.encode_image(image).cpu().numpy().flatten()\n",
    "        # Normalize the embedding\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to Get or Create Attribute ID\n",
    "def get_or_create_attribute(session, name, value):\n",
    "    instance = session.query(Attribute).filter_by(name=name, value=value).first()\n",
    "    if not instance:\n",
    "        instance = Attribute(name=name, value=value)\n",
    "        session.add(instance)\n",
    "        session.commit()\n",
    "    return instance.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac99072-0dc0-4ce2-a7c9-9fd97550ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Dimension of the Embeddings\n",
    "# d = 512  # Embedding dimension for CLIP ViT-B/32\n",
    "\n",
    "# # Collect Embeddings and Metadata\n",
    "# embeddings = []\n",
    "# image_ids = []\n",
    "# faiss_index = 0\n",
    "\n",
    "# metadata = metadata.head(5000)\n",
    "\n",
    "# # Wrap the loop with tqdm for progress tracking\n",
    "# for idx, row in tqdm(metadata.iterrows(), total=metadata.shape[0], desc=\"Processing images\"):\n",
    "#     image_path = os.path.join(images_folder, str(row['id'])+\".jpg\")\n",
    "#     image_path = os.path.normpath(image_path)  # Normalize the path to ensure consistency\n",
    "#     if os.path.exists(image_path):\n",
    "#         embedding = generate_embedding(image_path)\n",
    "#         if embedding is not None:\n",
    "#             embeddings.append(embedding)\n",
    "#             image_ids.append(row['id'])\n",
    "            \n",
    "#             faiss_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5fa9b0-5d9d-47df-b1aa-5f1c2b0c8d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|                                                                     | 0/44446 [00:00<?, ?it/s]C:\\Users\\jtren\\anaconda3\\envs\\LLM\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Processing images: 100%|█████████████████████████████████████████████████████████| 44446/44446 [27:32<00:00, 26.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the Dimension of the Embeddings\n",
    "d = 512  # Embedding dimension for CLIP ViT-B/32\n",
    "\n",
    "# Collect Embeddings and Metadata\n",
    "embeddings = []\n",
    "image_ids = []\n",
    "faiss_index = 0\n",
    "\n",
    "\n",
    "# Wrap the loop with tqdm for progress tracking\n",
    "for idx, row in tqdm(metadata.iterrows(), total=metadata.shape[0], desc=\"Processing images\"):\n",
    "    image_path = os.path.join(images_folder, str(row['id'])+\".jpg\")\n",
    "    image_path = os.path.normpath(image_path)  # Normalize the path to ensure consistency\n",
    "    if os.path.exists(image_path):\n",
    "        embedding = generate_embedding(image_path)\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)\n",
    "            image_ids.append(row['id'])\n",
    "\n",
    "            # Insert Image Metadata into the Database\n",
    "            image_instance = ImageModel(\n",
    "                image_path=image_path,\n",
    "                base_color=row.get('baseColour'),\n",
    "                season=row.get('season'),\n",
    "                article_type=row.get('articleType'),\n",
    "                faiss_index=faiss_index\n",
    "            )\n",
    "            session.add(image_instance)\n",
    "            session.commit()\n",
    "            \n",
    "            faiss_index += 1\n",
    "            \n",
    "            # Insert Attribute Data and Mapping\n",
    "            for attribute in required_columns:\n",
    "                value = row.get(attribute)\n",
    "                attribute_id = get_or_create_attribute(session, attribute, value)\n",
    "                item_attribute = ItemAttribute(image_id=image_instance.id, attribute_id=attribute_id)\n",
    "                session.add(item_attribute)\n",
    "            session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ebab7e-3d44-4371-b6f5-e59dff60ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Embeddings to Numpy Array and Save to Disk\n",
    "embeddings = np.vstack(embeddings).astype('float32')\n",
    "np.save(embeddings_file, embeddings)\n",
    "np.save(image_ids_file, image_ids)\n",
    "\n",
    "# Close the Session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bb48d-f2d5-479a-a2d2-f2a9edc90aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
